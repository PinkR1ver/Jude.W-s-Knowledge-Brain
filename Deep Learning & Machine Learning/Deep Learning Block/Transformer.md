---
 tags: deep-learning, attention
---

> [!info] 
> 在学习Transformer前，你需要学习 [[Deep Learning & Machine Learning/Deep Learning Block/⭐Attention]]



Transformer 是Seq2Seq model，由Encoder和Decoder组成
![[Deep Learning & Machine Learning/Deep Learning Block/attachments/Pasted image 20230316160103.png|300]]

# Encoder
这里贴的是原文Encoder的架构
![[Deep Learning & Machine Learning/Deep Learning Block/attachments/Pasted image 20230316162635.png]]

![[Deep Learning & Machine Learning/Deep Learning Block/attachments/Pasted image 20230316162642.png]]