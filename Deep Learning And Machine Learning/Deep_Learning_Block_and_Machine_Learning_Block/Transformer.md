---
title: "Transformer"
tags:
- deep-learning
- attention
---

> [!info] 
> 在学习Transformer前，你需要学习 [⭐Attention](Deep%20Learning%20And%20Machine%20Learning/Deep_Learning_Block_and_Machine_Learning_Block/⭐Attention.md)



Transformer 是Seq2Seq model，由Encoder和Decoder组成
![300](Deep%20Learning%20And%20Machine%20Learning/Deep_Learning_Block_and_Machine_Learning_Block/attachments/Pasted%20image%2020230316160103.png)

# Encoder
这里贴的是原文Encoder的架构
![Pasted image 20230316162635](Deep%20Learning%20And%20Machine%20Learning/Deep_Learning_Block_and_Machine_Learning_Block/attachments/Pasted%20image%2020230316162635.png)

![Pasted image 20230316162642](Deep%20Learning%20And%20Machine%20Learning/Deep_Learning_Block_and_Machine_Learning_Block/attachments/Pasted%20image%2020230316162642.png)